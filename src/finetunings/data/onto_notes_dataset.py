"""
 * Software Name: task-inclusion-estimation
 * SPDX-FileCopyrightText: Copyright (c) Orange SA
 * SPDX-License-Identifier: MIT
 *
 * This software is distributed under the MIT license,
 * the text of which is available at https://opensource.org/license/MIT/
 * or see the "LICENSE" file for more details.
"""
import os
import json
from datasets import Dataset
import fsspec

DATASET_SEED = 1234

class onto_notes_dataset(object):
    def __init__(
            self,
            debug: bool = False,
            data_dir: str = ".data",
            corpus_name: str = 'onto_json_name_summary_v1',
            neutral_prompt: bool = False,
            fsspec_protocol: str = "file"
    ):
        self.debug = debug
        self.fs = fsspec.filesystem(fsspec_protocol)
        self.corpus_name = os.path.join(data_dir, corpus_name)
        self.neutral_prompt = neutral_prompt
        self.name = ""

    def __call__(self, name: str = 'summary'):
        self.name = name
        if name in ["onto_notes_base_summary", "onto_notes_summary", "onto_notes_summary_10_pc", "onto_notes_summary_50_pc"]:
            return {
                'train': self.get_summary_part('train'),
                'validation': self.get_summary_part('validation'),
                'test': self.get_summary_part('test'),
            }
        elif name in ["onto_notes_named_entity", "onto_notes_named_entity_compress"]:
            return {
                'train': self.get_named_entity_task('train'),
                'validation': self.get_named_entity_task('validation'),
                'test': self.get_named_entity_task('test'),
            }
        else:
            return {
                'train': eval(f'self.get_{name.replace("onto_notes_","")}')('train', neutral_prompt=self.neutral_prompt),
                'validation': eval(f'self.get_{name.replace("onto_notes_","")}')('validation', neutral_prompt=self.neutral_prompt),
                'test': eval(f'self.get_{name.replace("onto_notes_","")}')('test', neutral_prompt=self.neutral_prompt),
            }

    #########################
    # --- SUMMARIZATION --- #
    #########################
    def get_summary_part(
            self,
            split: str = 'train',          
    ):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)
        texts = []
        summaries = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast conversations
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('.ipynb_checkpoints/'): continue
            if l.endswith('comment'): continue
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if 'id' in doc:
                    if doc['id'] not in documents: continue
                    ids.append(doc['id'])
                elif 'fileid' in doc:
                    if doc['fileid'] not in documents: continue
                    ids.append(doc['fileid'])
                else:
                    raise Exception
                if self.corpus_name == 'onto_json_name_summary_10pc_v2': # old 10% summaries
                    summaries.append(doc['summary'])
                    texts.append(doc['text'])
                elif self.corpus_name == 'corpus_onto_compress_1605':
                    if self.name == "onto_notes_summary_10_pc": # new 10% summaries
                        summaries.append(doc['content']['chatgpt'][0]['text'])
                        assert doc['content']['chatgpt'][-1]['desc'] == "summary generated by chatGPT with a target size of 50%"
                    elif self.name == "onto_notes_summary_50_pc": # 50% summaries
                        summaries.append(doc['content']['chatgpt'][-1]['text'])
                        assert doc['content']['chatgpt'][-1]['desc'] == "summary generated by chatGPT with a target size of 50%"
                    texts.append(doc['document'])
                elif self.corpus_name == 'onto_json_name_summary_v1':
                    summaries.append(doc['summary'])
                    text = []
                    for content in doc['task_ne']['content']:
                        text.append(content['in'])
                    texts.append(''.join(text))
                else:
                    raise NotImplementedError(f"`{self.name}` as a dataset name is not implemented.")
        data = Dataset.from_dict({
            'document': texts,
            'summary': summaries,
            'ids': ids,
        })
        def _mapping(batch):
            text = [
f"""Summarize the following document.

### Document:
{doc}

### Summary:
{sum}
""" for doc, sum in zip(batch['document'], batch['summary'])
            ]
            return {'text': text, 'document_ids': batch['ids']}
        return data.map(_mapping, batched=True, remove_columns=data.column_names)
    
    ####################################
    # --- NAMED ENTITY RECOGNITION --- #
    ####################################
    def get_named_entity_task(
            self,
            split: str = 'train',
    ):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        ner = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if 'id' in doc:
                    if doc['id'] not in documents: continue # next doc
                    ids.append(doc['id'])
                elif 'fileid' in doc:
                    if doc['fileid'] not in documents: continue
                    ids.append(doc['fileid'])
                else:
                    raise Exception
                
                if self.corpus_name == 'onto_json_name_summary_v1':
                    for content in doc['task_ne']['content']:
                        sentences.append(content['in'])
                        ner.append(content['out'][1])
                elif self.corpus_name == 'onto_json_name_summary_10pc_v2':
                    sentences.append(doc['text'])
                    ner.append(doc['task_ne']['content'][1])
                elif self.corpus_name == 'corpus_onto_compress_1605':
                    sentences.append(doc['document'])
                    ner.append(doc['content']['ne'][-1]['text'])
                    assert doc['content']['ne'][-1]['type'] == "compress:linguistic"
                else:
                    raise NotImplementedError(f'{self.corpus_name} does not exists as a corpus name')

        data = Dataset.from_dict({
            'sentence': sentences,
            'ner': ner,
            'ids': ids,
        })

        def _mapping(batch):
            text: str = None
            if self.corpus_name == 'onto_json_name_summary_v1':
                text = [
f"""What are the named entities in the following sentence ?

### Sentence:
{sent}

### Answer:
{ner}
""" for sent, ner in zip(batch['sentence'], batch['ner'])
                ]
            elif self.corpus_name == 'onto_json_name_summary_10pc_v2':
                text = [
f"""What are the named entities in the following document ?

### Document:
{sent}

### Answer:
{ner}
""" for sent, ner in zip(batch['sentence'], batch['ner'])
                ]
            elif self.corpus_name == "corpus_onto_compress_1605":
                text = [
f"""In the following document, keep only sentences containing at least one new Named Entity from the following categories: EVENT,FAC,GPE,LOC,ORG,PERSON,PRODUCT,WORK_OF_ART.

### Document:
{sent}

### Answer:
{ner}
""" for sent, ner in zip(batch['sentence'], batch['ner'])
                ]
            return {'text': text, 'document_ids': batch['ids']}
            
        return data.map(_mapping, batched=True, remove_columns=data.column_names)   

    #################
    # --- COREF --- #
    #################
    def get_coref_control_linguistic(self, split:str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        coreferences = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                for k in doc['content']['coref']:
                    if k['type'] == 'control:linguistic':
                        coreferences.append(k['text'])
        
        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, list the different mentions for each coreference chain.'

        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['coref'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'coref': coreferences,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
    

    def get_coref_compress_linguistic(self, split:str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        coreferences = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                for k in doc['content']['coref']:
                    if k['type'] == 'compress:linguistic':
                        coreferences.append(k['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep sentences with longest coreference chains and replace coreference with anchor.'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['coref'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'coref': coreferences,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
    

    ##################
    # --- syntax --- #
    ##################
    def get_syntax_control_linguistic(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        syntax = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                for k in doc['content']['syntax']:
                    if k['type'] == 'control:linguistic':
                        syntax.append(k['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, list the NP-SBJ syntagms.'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['syntax'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'syntax': syntax,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
    

    def get_syntax_compress_linguistic(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        syntax = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                for k in doc['content']['syntax']:
                    if k['type'] == 'compress:linguistic':
                        syntax.append(k['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, remove PP and AD-SBAR syntagms.'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['syntax'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'syntax': syntax,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
    

    ####################
    # --- semantic --- #
    ####################
    def get_semantic_control_linguistic(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        semantic = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                for k in doc['content']['semantic']:
                    if k['type'] == 'control:linguistic':
                        semantic.append(k['text'])
        
        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, list all predicates PRED(ARG0,ARG1).'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['semantic'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'semantic': semantic,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
    

    def get_semantic_compress_linguistic(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        semantic = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                for k in doc['content']['semantic']:
                    if k['type'] == 'compress:linguistic':
                        semantic.append(k['text'])
        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, build sentences with only ARG0 VERB ARG1 ARGM-TMP.'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['semantic'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'semantic': semantic,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
    



    ###########################
    # --- synthetic tasks --- #
    ###########################
    def get_format_first_10pc_wd(self, split:str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][0]
                assert 'keep only the first 10% words' in buff['desc'], 'problem in format_first_10pc_wd'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only the first 10% words'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)


    def get_format_last_10pc_wd(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][1]
                assert 'keep only the last 10% words' in buff['desc'], 'problem in format_last_10pc_wd'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only the last 10% words'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
        
    
    def get_format_1_every_10_wd(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][2]
                assert 'keep only 1 every 10 words' in buff['desc'], 'problem in format_last_1_every_10_wd'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only 1 every 10 words'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
        

    def get_format_1_every_2_wd(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][3]
                assert 'keep only 1 every 2 words' in buff['desc'], 'problem in format_1_every_2_wd'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only 1 every 2 words'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
        
    
    def get_format_first_10pc_sent(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][4]
                assert "keep only the first 10% sentences" in buff['desc'], 'problem in format_first_10pc_sent'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only the first 10 pourcent sentences'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
        
    
    def get_format_last_10pc_sent(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][5]
                assert "keep only the last 10% sentences" in buff['desc'], 'problem in format_last_10pc_sent'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only the last 10% sentences'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)

        

    def get_format_1_every_10_sent(self, split: str = 'train', neutral_prompt: bool = False):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][6]
                assert "keep only 1 every 10 sentences" in buff['desc'], 'problem in format_1_every_10_sent'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only 1 every 10 sentences'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)
        

    def get_format_1_every_2_sent(self, split: str = 'train', neutral_prompt: bool = True):
        documents = None
        with self.fs.open(self.corpus_name+f'/{split}_docs.json', 'r') as f:
            documents = json.load(f)

        sentences = []
        fm = []
        ids = []
        for l in self.fs.ls(self.corpus_name):
            if l.endswith('_bc.json'): continue              # skip broadcast news
            if l.endswith('_docs.json'): continue            # skip split files
            if l.endswith('comment'): continue               # skip comment files
            with self.fs.open(l, 'r') as f:
                corpus = json.load(f)
            for doc in corpus:
                if doc['fileid'] not in documents: continue # next doc
                ids.append(doc['fileid'])
                sentences.append(doc['document'])
                buff = doc['content']['format'][7]
                assert "keep only 1 every 2 sentences" in buff['desc'], 'problem in format_1_every_2_sent'
                fm.append(buff['text'])

        if neutral_prompt:
            prompt = 'Summarize the following document.'
        else:
            prompt = 'In the following document, keep only 1 every 2 sentences'
        
        def _mapping(batch):
            text = [
f"""{prompt}

### Document:
{d}

### Answer:
{c}
""" for d, c in zip(batch['sentence'], batch['format'])
            ]
            return {'text' : text, 'document_ids': batch['ids']}
        

        data = Dataset.from_dict({
            'sentence': sentences,
            'format': fm,
            'ids': ids,
        })

        return data.map(_mapping, batched=True, remove_columns=data.column_names)





        

        



